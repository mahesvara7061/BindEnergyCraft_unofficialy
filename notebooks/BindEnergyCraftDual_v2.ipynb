{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTagjooX0XC1"
      },
      "source": [
        "# BindCraft: Protein binder design\n",
        "\n",
        "<img src=\"https://github.com/martinpacesa/BindCraft/blob/main/pipeline.png?raw=true\">\n",
        "\n",
        "Simple binder design pipeline using AlphaFold2 backpropagation, MPNN, and PyRosetta. Select your target and let the script do the rest of the work and finish once you have enough designs to order!\n",
        "\n",
        "The designs will be saved on your Google Drive under BindCraft/[design_name]/ and you can continue running the design pipeline if the session times out and it will continue adding new designs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fMzl8JiyaXm",
        "outputId": "fbe4b8d6-a201-435e-eb63-d64b92a3543e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required BindCraft components\n",
            "Pulling BindCraft code from Github\n",
            "Cloning into '/content/bindcraft'...\n",
            "remote: Enumerating objects: 485, done.\u001b[K\n",
            "remote: Counting objects: 100% (296/296), done.\u001b[K\n",
            "remote: Compressing objects: 100% (96/96), done.\u001b[K\n",
            "remote: Total 485 (delta 261), reused 200 (delta 200), pack-reused 189 (from 1)\u001b[K\n",
            "Receiving objects: 100% (485/485), 5.67 MiB | 13.29 MiB/s, done.\n",
            "Resolving deltas: 100% (323/323), done.\n",
            "Installing ColabDesign\n",
            "Installing PyRosetta\n",
            "BindCraft installation is finished, ready to run!\n",
            "CPU times: user 1.52 s, sys: 182 ms, total: 1.71 s\n",
            "Wall time: 1min 33s\n"
          ]
        }
      ],
      "source": [
        "#@title Installation\n",
        "%%time\n",
        "import os, time, gc, io\n",
        "import contextlib\n",
        "import json\n",
        "from datetime import datetime\n",
        "from ipywidgets import HTML, VBox\n",
        "from IPython.display import display\n",
        "\n",
        "if not os.path.isfile(\"bindcraft/params/done.txt\"):\n",
        "  print(\"Installing required BindCraft components\")\n",
        "\n",
        "  print(\"Pulling BindCraft code from Github\")\n",
        "  os.makedirs('/content/bindcraft/', exist_ok=True)\n",
        "  !git clone https://github.com/mahesvara7061/BindEnergyCraft_unofficialy.git /content/bindcraft/\n",
        "  os.system(\"chmod +x /content/bindcraft/functions/dssp\")\n",
        "  os.system(\"chmod +x /content/bindcraft/functions/DAlphaBall.gcc\")\n",
        "\n",
        "  print(\"Installing ColabDesign\")\n",
        "  os.system(\"(mkdir bindcraft/params; apt-get install aria2 -qq; \\\n",
        "  aria2c -q -x 16 https://storage.googleapis.com/alphafold/alphafold_params_2022-12-06.tar; \\\n",
        "  tar -xf alphafold_params_2022-12-06.tar -C bindcraft/params; touch bindcraft/params/done.txt )&\")\n",
        "  os.system(\"pip install git+https://github.com/sokrypton/ColabDesign.git\")\n",
        "  # for debugging purposes\n",
        "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabdesign colabdesign\")\n",
        "\n",
        "  print(\"Installing PyRosetta\")\n",
        "  os.system(\"pip install pyrosettacolabsetup\")\n",
        "  with contextlib.redirect_stdout(io.StringIO()):\n",
        "    import pyrosettacolabsetup\n",
        "    pyrosettacolabsetup.install_pyrosetta(serialization=True, cache_wheel_on_google_drive=False)\n",
        "\n",
        "  # download params\n",
        "  if not os.path.isfile(\"bindcraft/params/done.txt\"):\n",
        "    print(\"downloading AlphaFold params\")\n",
        "    while not os.path.isfile(\"bindcraft/params/done.txt\"):\n",
        "      time.sleep(5)\n",
        "\n",
        "  print(\"BindCraft installation is finished, ready to run!\")\n",
        "else:\n",
        "  print(\"BindCraft components already installed, ready to run!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01IH64-ycCQY",
        "outputId": "49ca628d-a34e-499f-a6ac-421f256469b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google drive mounted at: 2025-10-31 08:57:54\n",
            "BindCraft folder successfully created in your drive!\n"
          ]
        }
      ],
      "source": [
        "#@title Mount your Google Drive to save design results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "currenttime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "print(f\"Google drive mounted at: {currenttime}\")\n",
        "\n",
        "bindcraft_google_drive = '/content/drive/My Drive/DualEnergyCraft/'\n",
        "os.makedirs(bindcraft_google_drive, exist_ok=True)\n",
        "print(\"BindCraft folder successfully created in your drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbL-S_t2hicj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11061c64-8981-42ee-e448-5db4b20b6d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binder design settings updated at: 2025-10-31 08:57:56\n",
            "New .json file with target settings has been generated in: /content/drive/MyDrive/DualEnergyCraft/PDL1/PDL1.json\n"
          ]
        }
      ],
      "source": [
        "#@title Binder design settings\n",
        "# @markdown ---\n",
        "# @markdown Enter path where to save your designs. We recommend to save on Google drive so that you can continue generating at any time.\n",
        "design_path = \"/content/drive/MyDrive/DualEnergyCraft/PDL1/\" # @param {\"type\":\"string\",\"placeholder\":\"/content/drive/MyDrive/BindCraft/PDL1/\"}\n",
        "\n",
        "# @markdown Enter the name that should be prefixed to your binders (generally target name).\n",
        "binder_name = \"PDL1\" # @param {\"type\":\"string\",\"placeholder\":\"PDL1\"}\n",
        "\n",
        "# @markdown The path to the .pdb structure of your target. Can be an experimental or AlphaFold2 structure. We recommend trimming the structure to as small as needed, as the whole selected chains will be backpropagated through the network and can significantly increase running times.\n",
        "starting_pdb = \"/content/bindcraft/example/dual_target.pdb\" # @param {\"type\":\"string\",\"placeholder\":\"/content/bindcraft/example/PDL1.pdb\"}\n",
        "\n",
        "# @markdown Which chains of your PDB to target? Can be one or multiple, in a comma-separated format. Other chains will be ignored during design.\n",
        "chains = \"A,B\" # @param {\"type\":\"string\",\"placeholder\":\"A,C\"}\n",
        "\n",
        "# @markdown What positions to target in your protein of interest? For example `1,2-10` or chain specific `A1-10,B1-20` or entire chains `A`. If left blank, an appropriate site will be selected by the pipeline.\n",
        "target_hotspot_residues = \"\" # @param {\"type\":\"string\",\"placeholder\":\"\"}\n",
        "\n",
        "# @markdown What is the minimum and maximum size of binders you want to design? Pipeline will randomly sample different sizes between these values.\n",
        "lengths = \"70,150\" # @param {\"type\":\"string\",\"placeholder\":\"70,150\"}\n",
        "\n",
        "# @markdown How many binder designs passing filters do you require?\n",
        "number_of_final_designs = 100 # @param {\"type\":\"integer\",\"placeholder\":\"100\"}\n",
        "# @markdown ---\n",
        "# @markdown Enter path on your Google drive (/content/drive/MyDrive/BindCraft/[binder_name].json) to previous target settings to continue design campaign. If left empty, it will use the settings above and generate a new settings json in your design output folder.\n",
        "load_previous_target_settings = \"\" # @param {\"type\":\"string\",\"placeholder\":\"\"}\n",
        "# @markdown ---\n",
        "\n",
        "if load_previous_target_settings:\n",
        "    target_settings_path = load_previous_target_settings\n",
        "else:\n",
        "    lengths = [int(x.strip()) for x in lengths.split(',') if len(lengths.split(',')) == 2]\n",
        "\n",
        "    if len(lengths) != 2:\n",
        "        raise ValueError(\"Incorrect specification of binder lengths.\")\n",
        "\n",
        "    settings = {\n",
        "        \"design_path\": design_path,\n",
        "        \"binder_name\": binder_name,\n",
        "        \"starting_pdb\": starting_pdb,\n",
        "        \"chains\": chains,\n",
        "        \"target_hotspot_residues\": target_hotspot_residues,\n",
        "        \"lengths\": lengths,\n",
        "        \"number_of_final_designs\": number_of_final_designs\n",
        "    }\n",
        "\n",
        "    target_settings_path = os.path.join(design_path, binder_name+\".json\")\n",
        "    os.makedirs(design_path, exist_ok=True)\n",
        "\n",
        "    with open(target_settings_path, 'w') as f:\n",
        "        json.dump(settings, f, indent=4)\n",
        "\n",
        "currenttime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "print(f\"Binder design settings updated at: {currenttime}\")\n",
        "print(f\"New .json file with target settings has been generated in: {target_settings_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcEjqCIlhire",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d13bed94-1c02-4852-f74b-a3b6a051f8da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advanced design settings updated at: 2025-10-31 08:57:57\n"
          ]
        }
      ],
      "source": [
        "#@title Advanced settings\n",
        "# @markdown ---\n",
        "# @markdown Which binder design protocol to run? Default is recommended. \"Beta-sheet\" promotes the design of more beta sheeted proteins, but requires more sampling. \"Peptide\" is optimised for helical peptide binders.\n",
        "design_protocol = \"Default\" # @param [\"Default\",\"Beta-sheet\",\"Peptide\"]\n",
        "# @markdown What prediction protocol to use?. \"Default\" performs single sequence prediction of the binder. \"HardTarget\" uses initial guess to improve complex prediction for difficult targets, but might introduce some bias.\n",
        "prediction_protocol = \"Default\" # @param [\"Default\",\"HardTarget\"]\n",
        "# @markdown What interface design method to use?. \"AlphaFold2\" is the default, interface is generated by AlphaFold2. \"MPNN\" uses soluble MPNN to optimise the interface.\n",
        "interface_protocol = \"AlphaFold2\" # @param [\"AlphaFold2\",\"MPNN\"]\n",
        "# @markdown What target template protocol to use? \"Default\" allows for limited amount flexibility. \"Masked\" allows for greater target flexibility on both sidechain and backbone level.\n",
        "template_protocol = \"Default\" # @param [\"Default\",\"Masked\"]\n",
        "# @markdown ---\n",
        "\n",
        "if design_protocol == \"Default\":\n",
        "    design_protocol_tag = \"default_4stage_multimer\"\n",
        "elif design_protocol == \"Beta-sheet\":\n",
        "    design_protocol_tag = \"betasheet_4stage_multimer\"\n",
        "elif design_protocol == \"Peptide\":\n",
        "    design_protocol_tag = \"peptide_3stage_multimer\"\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported design protocol\")\n",
        "\n",
        "if interface_protocol == \"AlphaFold2\":\n",
        "    interface_protocol_tag = \"\"\n",
        "elif interface_protocol == \"MPNN\":\n",
        "    interface_protocol_tag = \"_mpnn\"\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported interface protocol\")\n",
        "\n",
        "if template_protocol == \"Default\":\n",
        "    template_protocol_tag = \"\"\n",
        "elif template_protocol == \"Masked\":\n",
        "    template_protocol_tag = \"_flexible\"\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported template protocol\")\n",
        "\n",
        "if design_protocol in [\"Peptide\"]:\n",
        "    prediction_protocol_tag = \"\"\n",
        "else:\n",
        "    if prediction_protocol == \"Default\":\n",
        "        prediction_protocol_tag = \"\"\n",
        "    elif prediction_protocol == \"HardTarget\":\n",
        "        prediction_protocol_tag = \"_hardtarget\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported prediction protocol\")\n",
        "\n",
        "# advanced_settings_path = \"/content/bindcraft/settings_advanced/\" + design_protocol_tag + interface_protocol_tag + template_protocol_tag + prediction_protocol_tag + \".json\"\n",
        "advanced_settings_path = \"/content/bindcraft/settings_advanced/default_4stage_multimer_dual_ptme.json\"\n",
        "\n",
        "currenttime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "print(f\"Advanced design settings updated at: {currenttime}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r-OpCVe4hi5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c18464-a483-4140-f68a-a60d7307b1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filter settings updated at: 2025-10-31 08:57:57\n"
          ]
        }
      ],
      "source": [
        "#@title Filters\n",
        "# @markdown ---\n",
        "# @markdown Which filters for designs to use? \"Default\" are recommended, \"Peptide\" are for the design of peptide binders, \"Relaxed\" are more permissive but may result in fewer experimental successes, \"Peptide_Relaxed\" are more permissive filters for non-helical peptides, \"None\" is for benchmarking.\n",
        "filter_option = \"Default\" # @param [\"Default\", \"Peptide\", \"Relaxed\", \"Peptide_Relaxed\", \"None\"]\n",
        "# @markdown ---\n",
        "\n",
        "if filter_option == \"Default\":\n",
        "    filter_settings_path = \"/content/bindcraft/settings_filters/default_filters.json\"\n",
        "elif filter_option == \"Peptide\":\n",
        "    filter_settings_path = \"/content/bindcraft/settings_filters/peptide_filters.json\"\n",
        "elif filter_option == \"Relaxed\":\n",
        "    filter_settings_path = \"/content/bindcraft/settings_filters/relaxed_filters.json\"\n",
        "elif filter_option == \"Peptide_Relaxed\":\n",
        "    filter_settings_path = \"/content/bindcraft/settings_filters/peptide_relaxed_filters.json\"\n",
        "elif filter_option == \"None\":\n",
        "    filter_settings_path = \"/content/bindcraft/settings_filters/no_filters.json\"\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported filter type\")\n",
        "\n",
        "currenttime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "print(f\"Filter settings updated at: {currenttime}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR3gtmcChtvX"
      },
      "source": [
        "# Everything is set, BindCraft is ready to run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgRFO3EKAnM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d492aa73-0acc-4e97-8a78-4e1ceceea941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available GPUs:\n",
            "NVIDIA L41: gpu\n",
            "Loaded design functions and settings at: 2025-10-31 08:58:00\n"
          ]
        }
      ],
      "source": [
        "# @title Import functions and settings\n",
        "from bindcraft.functions import *\n",
        "\n",
        "args = {\"settings\":target_settings_path,\n",
        "        \"filters\":filter_settings_path,\n",
        "        \"advanced\":advanced_settings_path}\n",
        "\n",
        "# Check if JAX-capable GPU is available, otherwise exit\n",
        "check_jax_gpu()\n",
        "\n",
        "# perform checks of input setting files\n",
        "settings_path, filters_path, advanced_path = (args[\"settings\"], args[\"filters\"], args[\"advanced\"])\n",
        "\n",
        "### load settings from JSON\n",
        "target_settings, advanced_settings, filters = load_json_settings(settings_path, filters_path, advanced_path)\n",
        "\n",
        "settings_file = os.path.basename(settings_path).split('.')[0]\n",
        "filters_file = os.path.basename(filters_path).split('.')[0]\n",
        "advanced_file = os.path.basename(advanced_path).split('.')[0]\n",
        "\n",
        "### load AF2 model settings\n",
        "design_models, prediction_models, multimer_validation = load_af2_models(advanced_settings[\"use_multimer_design\"])\n",
        "\n",
        "### perform checks on advanced_settings\n",
        "bindcraft_folder = \"colab\"\n",
        "advanced_settings = perform_advanced_settings_check(advanced_settings, bindcraft_folder)\n",
        "\n",
        "### generate directories, design path names can be found within the function\n",
        "design_paths = generate_directories(target_settings[\"design_path\"])\n",
        "\n",
        "### generate dataframes\n",
        "trajectory_labels, design_labels, final_labels = generate_dataframe_labels()\n",
        "\n",
        "trajectory_csv = os.path.join(target_settings[\"design_path\"], 'trajectory_stats.csv')\n",
        "mpnn_csv = os.path.join(target_settings[\"design_path\"], 'mpnn_design_stats.csv')\n",
        "final_csv = os.path.join(target_settings[\"design_path\"], 'final_design_stats.csv')\n",
        "failure_csv = os.path.join(target_settings[\"design_path\"], 'failure_csv.csv')\n",
        "\n",
        "create_dataframe(trajectory_csv, trajectory_labels)\n",
        "create_dataframe(mpnn_csv, design_labels)\n",
        "create_dataframe(final_csv, final_labels)\n",
        "generate_filter_pass_csv(failure_csv, args[\"filters\"])\n",
        "\n",
        "currenttime = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "print(f\"Loaded design functions and settings at: {currenttime}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sOAn_xyEZKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20766af-1fa6-44b3-d640-e434ad65d519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┌───────────────────────────────────────────────────────────────────────────────┐\n",
            "│                                  PyRosetta-4                                  │\n",
            "│               Created in JHU by Sergey Lyskov and PyRosetta Team              │\n",
            "│               (C) Copyright Rosetta Commons Member Institutions               │\n",
            "│                                                                               │\n",
            "│ NOTE: USE OF PyRosetta FOR COMMERCIAL PURPOSES REQUIRES PURCHASE OF A LICENSE │\n",
            "│          See LICENSE.PyRosetta.md or email license@uw.edu for details         │\n",
            "└───────────────────────────────────────────────────────────────────────────────┘\n",
            "PyRosetta-4 2025 [Rosetta PyRosetta4.Release.python312.ubuntu.cxx11thread.serialization 2025.43+release.9fd5078c70dbd3d139526618074f384bba8cde37 2025-10-23T17:08:57] retrieved from: http://www.pyrosetta.org\n"
          ]
        }
      ],
      "source": [
        "#@title Initialise PyRosetta\n",
        "\n",
        "####################################\n",
        "####################################\n",
        "####################################\n",
        "### initialise PyRosetta\n",
        "pr.init(f'-ignore_unrecognized_res -ignore_zero_occupancy -mute all -holes:dalphaball {advanced_settings[\"dalphaball_path\"]} -corrections::beta_nov16 true -relax:default_repeats 1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fetch 2 PDBs from RCSB, pick chains, merge into one dual-target PDB (A,B)\n",
        "#@markdown Nhập mã PDB của 2 protein (ví dụ PD-L1/PD-1): `5J89` và `3RRQ` hoặc tuỳ bạn\n",
        "pdb_id_A = \"5J89\"  # @param {type:\"string\"}\n",
        "pdb_id_B = \"3RRQ\"  # @param {type:\"string\"}\n",
        "\n",
        "#@markdown Nếu để trống, script sẽ tự dùng **chain đầu tiên** của mỗi PDB\n",
        "chain_sel_A = \"\"   # @param {type:\"string\"}\n",
        "chain_sel_B = \"\"   # @param {type:\"string\"}\n",
        "\n",
        "#@markdown Dịch chuyển toàn bộ protein B để không chồng lên A (Å)\n",
        "shift_B_x = 50.0   # @param {type:\"number\"}\n",
        "shift_B_y = 0.0    # @param {type:\"number\"}\n",
        "shift_B_z = 0.0    # @param {type:\"number\"}\n",
        "\n",
        "#@markdown Đường dẫn file đầu ra (để dùng trong BindCraft/BECraft)\n",
        "output_pdb = \"/content/bindcraft/example/dual_target.pdb\"  # @param {type:\"string\"}\n",
        "\n",
        "import os, sys, io, gzip, shutil, requests\n",
        "from pathlib import Path\n",
        "from Bio.PDB import PDBParser, PDBIO, Select\n",
        "import numpy as np\n",
        "\n",
        "def fetch_rcsb_pdb(pdb_id: str, out_path: str) -> str:\n",
        "    \"\"\"Tải .pdb từ RCSB. Trả về đường dẫn file đã lưu; raise nếu lỗi.\"\"\"\n",
        "    pdb_id = pdb_id.strip().upper()\n",
        "    url = f\"https://files.rcsb.org/download/{pdb_id}.pdb\"\n",
        "    r = requests.get(url, timeout=30)\n",
        "    if r.status_code != 200 or len(r.text.strip()) == 0 or \"ATOM\" not in r.text:\n",
        "        raise RuntimeError(f\"Không tải được {pdb_id}.pdb (HTTP {r.status_code}). Thử PDB ID khác.\")\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    with open(out_path, \"w\") as f:\n",
        "        f.write(r.text)\n",
        "    return out_path\n",
        "\n",
        "def pick_first_chain(structure):\n",
        "    \"\"\"Lấy ID chain đầu tiên trong model 0.\"\"\"\n",
        "    model0 = list(structure.get_models())[0]\n",
        "    chains = list(model0.get_chains())\n",
        "    if not chains:\n",
        "        raise ValueError(\"PDB không có chain (không có ATOM?).\")\n",
        "    return chains[0].id\n",
        "\n",
        "def extract_chain(structure, chain_id):\n",
        "    \"\"\"Trả về đối tượng chain theo chain_id từ model 0.\"\"\"\n",
        "    model0 = list(structure.get_models())[0]\n",
        "    for ch in model0.get_chains():\n",
        "        if ch.id == chain_id:\n",
        "            return ch\n",
        "    raise ValueError(f\"Không tìm thấy chain '{chain_id}' trong PDB.\")\n",
        "\n",
        "class OnlyTheseChains(Select):\n",
        "    def __init__(self, allowed_ids):\n",
        "        self.allowed = set(allowed_ids)\n",
        "    def accept_chain(self, chain):\n",
        "        return 1 if chain.id in self.allowed else 0\n",
        "\n",
        "# B1) Tải PDB\n",
        "workdir = \"/content/dual_pdb_tmp\"\n",
        "os.makedirs(workdir, exist_ok=True)\n",
        "path_A = fetch_rcsb_pdb(pdb_id_A, os.path.join(workdir, f\"{pdb_id_A}.pdb\"))\n",
        "path_B = fetch_rcsb_pdb(pdb_id_B, os.path.join(workdir, f\"{pdb_id_B}.pdb\"))\n",
        "\n",
        "# B2) Đọc & chọn chain\n",
        "parser = PDBParser(QUIET=True)\n",
        "structA = parser.get_structure(\"A\", path_A)\n",
        "structB = parser.get_structure(\"B\", path_B)\n",
        "\n",
        "if not chain_sel_A:\n",
        "    chain_sel_A = pick_first_chain(structA)\n",
        "if not chain_sel_B:\n",
        "    chain_sel_B = pick_first_chain(structB)\n",
        "\n",
        "# B3) Tạo structure mới: 1 model, 2 chain (đặt lại id thành 'A' và 'B')\n",
        "from Bio.PDB.Structure import Structure\n",
        "from Bio.PDB.Model import Model\n",
        "from Bio.PDB.Chain import Chain\n",
        "from Bio.PDB.Residue import Residue\n",
        "from Bio.PDB.Atom import Atom\n",
        "\n",
        "def clone_chain_as(chain, new_id):\n",
        "    \"\"\"Clone thô sơ 1 chain sang chain id mới (sao chép residue + CA/ATOM đầy đủ).\"\"\"\n",
        "    new_chain = Chain(new_id)\n",
        "    for res in chain:\n",
        "        # bỏ HETATM không chuẩn nếu cần (tuỳ chọn). Ở đây giữ nguyên mọi residue.\n",
        "        new_res = Residue(res.id, res.resname, res.segid)\n",
        "        for atom in res:\n",
        "            new_atom = Atom(atom.name, atom.coord.copy(), atom.bfactor, atom.occupancy,\n",
        "                            atom.altloc, atom.fullname, atom.serial_number, element=atom.element)\n",
        "            new_res.add(new_atom)\n",
        "        new_chain.add(new_res)\n",
        "    return new_chain\n",
        "\n",
        "# lấy chain gốc\n",
        "chainA_orig = extract_chain(structA, chain_sel_A)\n",
        "chainB_orig = extract_chain(structB, chain_sel_B)\n",
        "\n",
        "# clone + đặt id\n",
        "chainA_new = clone_chain_as(chainA_orig, \"A\")\n",
        "chainB_new = clone_chain_as(chainB_orig, \"B\")\n",
        "\n",
        "# dịch chuyển toàn bộ chain B\n",
        "shift_vec = np.array([shift_B_x, shift_B_y, shift_B_z], dtype=float)\n",
        "for res in chainB_new:\n",
        "    for atom in res:\n",
        "        atom.coord = atom.coord + shift_vec\n",
        "\n",
        "# ráp structure\n",
        "combined = Structure(\"COMBINED\")\n",
        "model0 = Model(0)\n",
        "model0.add(chainA_new)\n",
        "model0.add(chainB_new)\n",
        "combined.add(model0)\n",
        "\n",
        "# B4) Lưu file\n",
        "os.makedirs(os.path.dirname(output_pdb), exist_ok=True)\n",
        "io = PDBIO()\n",
        "io.set_structure(combined)\n",
        "io.save(output_pdb)\n",
        "\n",
        "# B5) In thông tin nhanh\n",
        "def chain_len(chain):\n",
        "    # đếm residue có ít nhất 1 ATOM\n",
        "    return sum(1 for _ in chain.get_residues())\n",
        "\n",
        "chains_found = [ch.id for ch in model0.get_chains()]\n",
        "lenA = chain_len(chainA_new)\n",
        "lenB = chain_len(chainB_new)\n",
        "\n",
        "print(\"✅ Saved:\", output_pdb)\n",
        "print(\"Chains in combined:\", chains_found)\n",
        "print(f\"Chain A length (residues): {lenA}\")\n",
        "print(f\"Chain B length (residues): {lenB}\")\n",
        "print(\"Gợi ý cấu hình:\")\n",
        "print('  starting_pdb = \"{}\"'.format(output_pdb))\n",
        "print('  chains = \"A,B\"')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLgC0lS449FJ",
        "outputId": "78d13362-9c20-4fcc-8847-3c037c9d47ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: /content/bindcraft/example/dual_target.pdb\n",
            "Chains in combined: ['A', 'B']\n",
            "Chain A length (residues): 192\n",
            "Chain B length (residues): 143\n",
            "Gợi ý cấu hình:\n",
            "  starting_pdb = \"/content/bindcraft/example/dual_target.pdb\"\n",
            "  chains = \"A,B\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH2hVVrpzn-o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684,
          "referenced_widgets": [
            "ff7d39e211514b43bc64bf39a422c3a3",
            "2b87c8a60a89479085437fa7414cfc57",
            "a9b21b0c30d644669ebf9fc1bf4fbc53",
            "bd842dacd993410f936fbdfc549b2db8",
            "cd6626b22753425dba9a91ced5ae562f",
            "db2ec6b24eb44a53b1a8bb36c3a11f5a",
            "916c0b73c7b64624a982b7da54df1381",
            "455fd818088a4085a4d03d8807005172"
          ]
        },
        "outputId": "3fb2f70c-2311-43fa-ba70-a5beb242dfdf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value=\"<h3 style='color: #1f77b4;'>Sampled Trajectories: <span style='color: #1f77b4;'>2</…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff7d39e211514b43bc64bf39a422c3a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting trajectory: PDL1_l78_s328526\n",
            "Stage 1: Test Logits\n",
            "1 models [3] recycles 1 hard 0 soft 0.02 temp 1 loss 18.10 helix 1.37 pae 0.84 i_pae 0.87 con 4.61 i_con 4.30 plddt 0.28 ptm 0.43 i_ptm 0.15 geo_sep 1.84 multi_ptme 0.13 overlap 0.20 ptme_A 2.45 ptme_B 2.72 rg 7.35 tau_current 0.50 weight_A 0.37 weight_B 0.63\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot convert float NaN to integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-378132694.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m### Begin binder hallucination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         trajectory = binder_hallucination(design_name, target_settings[\"starting_pdb\"], target_settings[\"chains\"],\n\u001b[0m\u001b[1;32m     56\u001b[0m                                             \u001b[0mtarget_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_hotspot_residues\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelicity_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                                             design_models, advanced_settings, design_paths, failure_csv)\n",
            "\u001b[0;32m/content/bindcraft/functions/colabdesign_utils.py\u001b[0m in \u001b[0;36mbinder_hallucination\u001b[0;34m(design_name, starting_pdb, chain, target_hotspot_residues, length, seed, helicity_value, design_models, advanced_settings, design_paths, failure_csv)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# initial logits to prescreen trajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stage 1: Test Logits\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0maf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesign_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_soft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesign_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madvanced_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_models\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# determine pLDDT of best iteration according to lowest 'loss' value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/colabdesign/af/design.py\u001b[0m in \u001b[0;36mdesign_logits\u001b[0;34m(self, iters, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdesign_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;34m''' optimize logits '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdesign_soft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/colabdesign/af/design.py\u001b[0m in \u001b[0;36mdesign\u001b[0;34m(self, iters, soft, e_soft, temp, e_temp, hard, e_hard, step, e_step, dropout, opt, weights, num_recycles, ramp_recycles, num_models, sample_models, models, backprop, callback, save_best, verbose)\u001b[0m\n\u001b[1;32m    346\u001b[0m       \u001b[0mlr_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"soft\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"soft\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"temp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m       self.step(lr_scale=lr_scale, num_recycles=num_recycles,\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 backprop=backprop, callback=callback, save_best=save_best, verbose=verbose)\n",
            "\u001b[0;32m/content/colabdesign/af/design.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, lr_scale, num_recycles, num_models, sample_models, models, backprop, callback, save_best, verbose)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_best\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/colabdesign/af/design.py\u001b[0m in \u001b[0;36m_save_results\u001b[0;34m(self, aux, save_best, best_metric, metric_higher_better, verbose)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_k\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self._k+1}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   def predict(self, seq=None, bias=None,\n",
            "\u001b[0;32m/content/colabdesign/af/design.py\u001b[0m in \u001b[0;36m_print_log\u001b[0;34m(self, print_str, aux)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0maux\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i_ptm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     print(dict_to_str(aux[\"log\"], filt=self.opt[\"weights\"],\n\u001b[0m\u001b[1;32m    244\u001b[0m                       print_str=print_str, keys=keys+[\"rmsd\"], ok=[\"plddt\",\"rmsd\"]))\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/colabdesign/shared/utils.py\u001b[0m in \u001b[0;36mdict_to_str\u001b[0;34m(x, filt, keys, ok, print_str, f)\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m           \u001b[0mprint_str\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" {k} {int(v)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
          ]
        }
      ],
      "source": [
        "#@title Run BindCraft!\n",
        "####################################\n",
        "###################### BindCraft Run\n",
        "####################################\n",
        "# Colab-specific: live displays\n",
        "num_sampled_trajectories = len(pd.read_csv(trajectory_csv))\n",
        "num_accepted_designs = len(pd.read_csv(final_csv))\n",
        "sampled_trajectories_label = HTML(value=f\"<h3 style='color: #1f77b4;'>Sampled Trajectories: <span style='color: #1f77b4;'>{num_sampled_trajectories}</span></h3>\")\n",
        "accepted_designs_label = HTML(value=f\"<h3 style='color: #2ca02c;'>Accepted Designs: <span style='color: #2ca02c;'>{num_accepted_designs}</span></h3>\")\n",
        "display(VBox([sampled_trajectories_label, accepted_designs_label]))\n",
        "\n",
        "# initialise counters\n",
        "script_start_time = time.time()\n",
        "trajectory_n = 1\n",
        "accepted_designs = 0\n",
        "\n",
        "### start design loop\n",
        "while True:\n",
        "    ### check if we have the target number of binders\n",
        "    final_designs_reached = check_accepted_designs(design_paths, mpnn_csv, final_labels, final_csv, advanced_settings, target_settings, design_labels)\n",
        "\n",
        "    if final_designs_reached:\n",
        "        # stop design loop execution\n",
        "        break\n",
        "\n",
        "    ### check if we reached maximum allowed trajectories\n",
        "    max_trajectories_reached = check_n_trajectories(design_paths, advanced_settings)\n",
        "\n",
        "    if max_trajectories_reached:\n",
        "        break\n",
        "\n",
        "    ### Initialise design\n",
        "    # measure time to generate design\n",
        "    trajectory_start_time = time.time()\n",
        "\n",
        "    # generate random seed to vary designs\n",
        "    seed = int(np.random.randint(0, high=999999, size=1, dtype=int)[0])\n",
        "\n",
        "    # sample binder design length randomly from defined distribution\n",
        "    samples = np.arange(min(target_settings[\"lengths\"]), max(target_settings[\"lengths\"]) + 1)\n",
        "    length = np.random.choice(samples)\n",
        "\n",
        "    # load desired helicity value to sample different secondary structure contents\n",
        "    helicity_value = load_helicity(advanced_settings)\n",
        "\n",
        "    # generate design name and check if same trajectory was already run\n",
        "    design_name = target_settings[\"binder_name\"] + \"_l\" + str(length) + \"_s\"+ str(seed)\n",
        "    trajectory_dirs = [\"Trajectory\", \"Trajectory/Relaxed\", \"Trajectory/LowConfidence\", \"Trajectory/Clashing\"]\n",
        "    trajectory_exists = any(os.path.exists(os.path.join(design_paths[trajectory_dir], design_name + \".pdb\")) for trajectory_dir in trajectory_dirs)\n",
        "\n",
        "    if not trajectory_exists:\n",
        "        print(\"Starting trajectory: \"+design_name)\n",
        "\n",
        "        ### Begin binder hallucination\n",
        "        trajectory = binder_hallucination(design_name, target_settings[\"starting_pdb\"], target_settings[\"chains\"],\n",
        "                                            target_settings[\"target_hotspot_residues\"], length, seed, helicity_value,\n",
        "                                            design_models, advanced_settings, design_paths, failure_csv)\n",
        "        trajectory_metrics = copy_dict(trajectory._tmp[\"best\"][\"aux\"][\"log\"]) # contains plddt, ptm, i_ptm, pae, i_pae\n",
        "        trajectory_pdb = os.path.join(design_paths[\"Trajectory\"], design_name + \".pdb\")\n",
        "\n",
        "        # round the metrics to two decimal places\n",
        "        trajectory_metrics = {k: round(v, 2) if isinstance(v, float) else v for k, v in trajectory_metrics.items()}\n",
        "\n",
        "        # time trajectory\n",
        "        trajectory_time = time.time() - trajectory_start_time\n",
        "        trajectory_time_text = f\"{'%d hours, %d minutes, %d seconds' % (int(trajectory_time // 3600), int((trajectory_time % 3600) // 60), int(trajectory_time % 60))}\"\n",
        "        print(\"Starting trajectory took: \"+trajectory_time_text)\n",
        "        print(\"\")\n",
        "\n",
        "        # Proceed if there is no trajectory termination signal\n",
        "        if trajectory.aux[\"log\"][\"terminate\"] == \"\":\n",
        "            # Relax binder to calculate statistics\n",
        "            trajectory_relaxed = os.path.join(design_paths[\"Trajectory/Relaxed\"], design_name + \".pdb\")\n",
        "            pr_relax(trajectory_pdb, trajectory_relaxed)\n",
        "\n",
        "            # define binder chain, placeholder in case multi-chain parsing in ColabDesign gets changed\n",
        "            binder_chain = \"B\"\n",
        "\n",
        "            # Calculate clashes before and after relaxation\n",
        "            num_clashes_trajectory = calculate_clash_score(trajectory_pdb)\n",
        "            num_clashes_relaxed = calculate_clash_score(trajectory_relaxed)\n",
        "\n",
        "            # secondary structure content of starting trajectory binder and interface\n",
        "            trajectory_alpha, trajectory_beta, trajectory_loops, trajectory_alpha_interface, trajectory_beta_interface, trajectory_loops_interface, trajectory_i_plddt, trajectory_ss_plddt = calc_ss_percentage(trajectory_pdb, advanced_settings, binder_chain)\n",
        "\n",
        "            # analyze interface scores for relaxed af2 trajectory\n",
        "            trajectory_interface_scores, trajectory_interface_AA, trajectory_interface_residues = score_interface(trajectory_relaxed, binder_chain)\n",
        "\n",
        "            # starting binder sequence\n",
        "            trajectory_sequence = trajectory.get_seq(get_best=True)[0]\n",
        "\n",
        "            # analyze sequence\n",
        "            traj_seq_notes = validate_design_sequence(trajectory_sequence, num_clashes_relaxed, advanced_settings)\n",
        "\n",
        "            # target structure RMSD compared to input PDB\n",
        "            trajectory_target_rmsd = unaligned_rmsd(target_settings[\"starting_pdb\"], trajectory_pdb, target_settings[\"chains\"], 'A')\n",
        "\n",
        "            # save trajectory statistics into CSV\n",
        "            trajectory_data = [design_name, advanced_settings[\"design_algorithm\"], length, seed, helicity_value, target_settings[\"target_hotspot_residues\"], trajectory_sequence, trajectory_interface_residues,\n",
        "                                trajectory_metrics['plddt'], trajectory_metrics['ptm'], trajectory_metrics['i_ptm'], trajectory_metrics['pae'], trajectory_metrics['i_pae'],\n",
        "                                trajectory_i_plddt, trajectory_ss_plddt, num_clashes_trajectory, num_clashes_relaxed, trajectory_interface_scores['binder_score'],\n",
        "                                trajectory_interface_scores['surface_hydrophobicity'], trajectory_interface_scores['interface_sc'], trajectory_interface_scores['interface_packstat'],\n",
        "                                trajectory_interface_scores['interface_dG'], trajectory_interface_scores['interface_dSASA'], trajectory_interface_scores['interface_dG_SASA_ratio'],\n",
        "                                trajectory_interface_scores['interface_fraction'], trajectory_interface_scores['interface_hydrophobicity'], trajectory_interface_scores['interface_nres'], trajectory_interface_scores['interface_interface_hbonds'],\n",
        "                                trajectory_interface_scores['interface_hbond_percentage'], trajectory_interface_scores['interface_delta_unsat_hbonds'], trajectory_interface_scores['interface_delta_unsat_hbonds_percentage'],\n",
        "                                trajectory_alpha_interface, trajectory_beta_interface, trajectory_loops_interface, trajectory_alpha, trajectory_beta, trajectory_loops, trajectory_interface_AA, trajectory_target_rmsd,\n",
        "                                trajectory_time_text, traj_seq_notes, settings_file, filters_file, advanced_file]\n",
        "            insert_data(trajectory_csv, trajectory_data)\n",
        "\n",
        "            if advanced_settings[\"enable_mpnn\"]:\n",
        "                # initialise MPNN counters\n",
        "                mpnn_n = 1\n",
        "                accepted_mpnn = 0\n",
        "                mpnn_dict = {}\n",
        "                design_start_time = time.time()\n",
        "\n",
        "                ### MPNN redesign of starting binder\n",
        "                mpnn_trajectories = mpnn_gen_sequence(trajectory_pdb, binder_chain, trajectory_interface_residues, advanced_settings)\n",
        "                existing_mpnn_sequences = set(pd.read_csv(mpnn_csv, usecols=['Sequence'])['Sequence'].values)\n",
        "\n",
        "                # create set of MPNN sequences with allowed amino acid composition\n",
        "                restricted_AAs = set(aa.strip().upper() for aa in advanced_settings[\"omit_AAs\"].split(',')) if advanced_settings[\"force_reject_AA\"] else set()\n",
        "\n",
        "                mpnn_sequences = sorted({\n",
        "                    mpnn_trajectories['seq'][n][-length:]: {\n",
        "                        'seq': mpnn_trajectories['seq'][n][-length:],\n",
        "                        'score': mpnn_trajectories['score'][n],\n",
        "                        'seqid': mpnn_trajectories['seqid'][n]\n",
        "                    } for n in range(advanced_settings[\"num_seqs\"])\n",
        "                    if (not restricted_AAs or not any(aa in mpnn_trajectories['seq'][n][-length:].upper() for aa in restricted_AAs))\n",
        "                    and mpnn_trajectories['seq'][n][-length:] not in existing_mpnn_sequences\n",
        "                }.values(), key=lambda x: x['score'])\n",
        "\n",
        "                del existing_mpnn_sequences\n",
        "\n",
        "                # check whether any sequences are left after amino acid rejection and duplication check, and if yes proceed with prediction\n",
        "                if mpnn_sequences:\n",
        "                    # add optimisation for increasing recycles if trajectory is beta sheeted\n",
        "                    if advanced_settings[\"optimise_beta\"] and float(trajectory_beta) > 15:\n",
        "                        advanced_settings[\"num_recycles_validation\"] = advanced_settings[\"optimise_beta_recycles_valid\"]\n",
        "\n",
        "                    ### Compile prediction models once for faster prediction of MPNN sequences\n",
        "                    clear_mem()\n",
        "                    # compile complex prediction model\n",
        "                    complex_prediction_model = mk_afdesign_model(protocol=\"binder\", num_recycles=advanced_settings[\"num_recycles_validation\"], data_dir=advanced_settings[\"af_params_dir\"],\n",
        "                                                                use_multimer=multimer_validation)\n",
        "                    complex_prediction_model.prep_inputs(pdb_filename=target_settings[\"starting_pdb\"], chain=target_settings[\"chains\"], binder_len=length, rm_target_seq=advanced_settings[\"rm_template_seq_predict\"],\n",
        "                                                        rm_target_sc=advanced_settings[\"rm_template_sc_predict\"])\n",
        "\n",
        "                    # compile binder monomer prediction model\n",
        "                    binder_prediction_model = mk_afdesign_model(protocol=\"hallucination\", use_templates=False, initial_guess=False,\n",
        "                                                                use_initial_atom_pos=False, num_recycles=advanced_settings[\"num_recycles_validation\"],\n",
        "                                                                data_dir=advanced_settings[\"af_params_dir\"], use_multimer=multimer_validation)\n",
        "                    binder_prediction_model.prep_inputs(length=length)\n",
        "\n",
        "                    # iterate over designed sequences\n",
        "                    for mpnn_sequence in mpnn_sequences:\n",
        "                        mpnn_time = time.time()\n",
        "\n",
        "                        # generate mpnn design name numbering\n",
        "                        mpnn_design_name = design_name + \"_mpnn\" + str(mpnn_n)\n",
        "                        mpnn_score = round(mpnn_sequence['score'],2)\n",
        "                        mpnn_seqid = round(mpnn_sequence['seqid'],2)\n",
        "\n",
        "                        # add design to dictionary\n",
        "                        mpnn_dict[mpnn_design_name] = {'seq': mpnn_sequence['seq'], 'score': mpnn_score, 'seqid': mpnn_seqid}\n",
        "\n",
        "                        # save fasta sequence\n",
        "                        if advanced_settings[\"save_mpnn_fasta\"] is True:\n",
        "                            save_fasta(mpnn_design_name, mpnn_sequence['seq'], design_paths)\n",
        "\n",
        "                        ### Predict mpnn redesigned binder complex using masked templates\n",
        "                        mpnn_complex_statistics, pass_af2_filters = predict_binder_complex(complex_prediction_model,\n",
        "                                                                                        mpnn_sequence['seq'], mpnn_design_name,\n",
        "                                                                                        target_settings[\"starting_pdb\"], target_settings[\"chains\"],\n",
        "                                                                                        length, trajectory_pdb, prediction_models, advanced_settings,\n",
        "                                                                                        filters, design_paths, failure_csv)\n",
        "\n",
        "                        # if AF2 filters are not passed then skip the scoring\n",
        "                        if not pass_af2_filters:\n",
        "                            print(f\"Base AF2 filters not passed for {mpnn_design_name}, skipping interface scoring\")\n",
        "                            mpnn_n += 1\n",
        "                            continue\n",
        "\n",
        "                        # calculate statistics for each model individually\n",
        "                        for model_num in prediction_models:\n",
        "                            mpnn_design_pdb = os.path.join(design_paths[\"MPNN\"], f\"{mpnn_design_name}_model{model_num+1}.pdb\")\n",
        "                            mpnn_design_relaxed = os.path.join(design_paths[\"MPNN/Relaxed\"], f\"{mpnn_design_name}_model{model_num+1}.pdb\")\n",
        "\n",
        "                            if os.path.exists(mpnn_design_pdb):\n",
        "                                # Calculate clashes before and after relaxation\n",
        "                                num_clashes_mpnn = calculate_clash_score(mpnn_design_pdb)\n",
        "                                num_clashes_mpnn_relaxed = calculate_clash_score(mpnn_design_relaxed)\n",
        "\n",
        "                                # analyze interface scores for relaxed af2 trajectory\n",
        "                                mpnn_interface_scores, mpnn_interface_AA, mpnn_interface_residues = score_interface(mpnn_design_relaxed, binder_chain)\n",
        "\n",
        "                                # secondary structure content of starting trajectory binder\n",
        "                                mpnn_alpha, mpnn_beta, mpnn_loops, mpnn_alpha_interface, mpnn_beta_interface, mpnn_loops_interface, mpnn_i_plddt, mpnn_ss_plddt = calc_ss_percentage(mpnn_design_pdb, advanced_settings, binder_chain)\n",
        "\n",
        "                                # unaligned RMSD calculate to determine if binder is in the designed binding site\n",
        "                                rmsd_site = unaligned_rmsd(trajectory_pdb, mpnn_design_pdb, binder_chain, binder_chain)\n",
        "\n",
        "                                # calculate RMSD of target compared to input PDB\n",
        "                                target_rmsd = target_pdb_rmsd(mpnn_design_pdb, target_settings[\"starting_pdb\"], target_settings[\"chains\"])\n",
        "\n",
        "                                # add the additional statistics to the mpnn_complex_statistics dictionary\n",
        "                                mpnn_complex_statistics[model_num+1].update({\n",
        "                                    'i_pLDDT': mpnn_i_plddt,\n",
        "                                    'ss_pLDDT': mpnn_ss_plddt,\n",
        "                                    'Unrelaxed_Clashes': num_clashes_mpnn,\n",
        "                                    'Relaxed_Clashes': num_clashes_mpnn_relaxed,\n",
        "                                    'Binder_Energy_Score': mpnn_interface_scores['binder_score'],\n",
        "                                    'Surface_Hydrophobicity': mpnn_interface_scores['surface_hydrophobicity'],\n",
        "                                    'ShapeComplementarity': mpnn_interface_scores['interface_sc'],\n",
        "                                    'PackStat': mpnn_interface_scores['interface_packstat'],\n",
        "                                    'dG': mpnn_interface_scores['interface_dG'],\n",
        "                                    'dSASA': mpnn_interface_scores['interface_dSASA'],\n",
        "                                    'dG/dSASA': mpnn_interface_scores['interface_dG_SASA_ratio'],\n",
        "                                    'Interface_SASA_%': mpnn_interface_scores['interface_fraction'],\n",
        "                                    'Interface_Hydrophobicity': mpnn_interface_scores['interface_hydrophobicity'],\n",
        "                                    'n_InterfaceResidues': mpnn_interface_scores['interface_nres'],\n",
        "                                    'n_InterfaceHbonds': mpnn_interface_scores['interface_interface_hbonds'],\n",
        "                                    'InterfaceHbondsPercentage': mpnn_interface_scores['interface_hbond_percentage'],\n",
        "                                    'n_InterfaceUnsatHbonds': mpnn_interface_scores['interface_delta_unsat_hbonds'],\n",
        "                                    'InterfaceUnsatHbondsPercentage': mpnn_interface_scores['interface_delta_unsat_hbonds_percentage'],\n",
        "                                    'InterfaceAAs': mpnn_interface_AA,\n",
        "                                    'Interface_Helix%': mpnn_alpha_interface,\n",
        "                                    'Interface_BetaSheet%': mpnn_beta_interface,\n",
        "                                    'Interface_Loop%': mpnn_loops_interface,\n",
        "                                    'Binder_Helix%': mpnn_alpha,\n",
        "                                    'Binder_BetaSheet%': mpnn_beta,\n",
        "                                    'Binder_Loop%': mpnn_loops,\n",
        "                                    'Hotspot_RMSD': rmsd_site,\n",
        "                                    'Target_RMSD': target_rmsd\n",
        "                                })\n",
        "\n",
        "                                # save space by removing unrelaxed predicted mpnn complex pdb?\n",
        "                                if advanced_settings[\"remove_unrelaxed_complex\"]:\n",
        "                                    os.remove(mpnn_design_pdb)\n",
        "\n",
        "                        # calculate complex averages\n",
        "                        mpnn_complex_averages = calculate_averages(mpnn_complex_statistics, handle_aa=True)\n",
        "\n",
        "                        ### Predict binder alone in single sequence mode\n",
        "                        binder_statistics = predict_binder_alone(binder_prediction_model, mpnn_sequence['seq'], mpnn_design_name, length,\n",
        "                                                                trajectory_pdb, binder_chain, prediction_models, advanced_settings, design_paths)\n",
        "\n",
        "                        # extract RMSDs of binder to the original trajectory\n",
        "                        for model_num in prediction_models:\n",
        "                            mpnn_binder_pdb = os.path.join(design_paths[\"MPNN/Binder\"], f\"{mpnn_design_name}_model{model_num+1}.pdb\")\n",
        "\n",
        "                            if os.path.exists(mpnn_binder_pdb):\n",
        "                                rmsd_binder = unaligned_rmsd(trajectory_pdb, mpnn_binder_pdb, binder_chain, \"A\")\n",
        "\n",
        "                            # append to statistics\n",
        "                            binder_statistics[model_num+1].update({\n",
        "                                    'Binder_RMSD': rmsd_binder\n",
        "                                })\n",
        "\n",
        "                            # save space by removing binder monomer models?\n",
        "                            if advanced_settings[\"remove_binder_monomer\"]:\n",
        "                                os.remove(mpnn_binder_pdb)\n",
        "\n",
        "                        # calculate binder averages\n",
        "                        binder_averages = calculate_averages(binder_statistics)\n",
        "\n",
        "                        # analyze sequence to make sure there are no cysteins and it contains residues that absorb UV for detection\n",
        "                        seq_notes = validate_design_sequence(mpnn_sequence['seq'], mpnn_complex_averages.get('Relaxed_Clashes', None), advanced_settings)\n",
        "\n",
        "                        # measure time to generate design\n",
        "                        mpnn_end_time = time.time() - mpnn_time\n",
        "                        elapsed_mpnn_text = f\"{'%d hours, %d minutes, %d seconds' % (int(mpnn_end_time // 3600), int((mpnn_end_time % 3600) // 60), int(mpnn_end_time % 60))}\"\n",
        "\n",
        "\n",
        "                        # Insert statistics about MPNN design into CSV, will return None if corresponding model does note exist\n",
        "                        model_numbers = range(1, 6)\n",
        "                        statistics_labels = ['pLDDT', 'pTM', 'i_pTM', 'pTME', 'pAE', 'i_pAE', 'i_pLDDT', 'ss_pLDDT', 'Unrelaxed_Clashes', 'Relaxed_Clashes', 'Binder_Energy_Score', 'Surface_Hydrophobicity',\n",
        "                                            'ShapeComplementarity', 'PackStat', 'dG', 'dSASA', 'dG/dSASA', 'Interface_SASA_%', 'Interface_Hydrophobicity', 'n_InterfaceResidues', 'n_InterfaceHbonds', 'InterfaceHbondsPercentage',\n",
        "                                            'n_InterfaceUnsatHbonds', 'InterfaceUnsatHbondsPercentage', 'Interface_Helix%', 'Interface_BetaSheet%', 'Interface_Loop%', 'Binder_Helix%',\n",
        "                                            'Binder_BetaSheet%', 'Binder_Loop%', 'InterfaceAAs', 'Hotspot_RMSD', 'Target_RMSD']\n",
        "\n",
        "                        # Initialize mpnn_data with the non-statistical data\n",
        "                        mpnn_data = [mpnn_design_name, advanced_settings[\"design_algorithm\"], length, seed, helicity_value, target_settings[\"target_hotspot_residues\"], mpnn_sequence['seq'], mpnn_interface_residues, mpnn_score, mpnn_seqid]\n",
        "\n",
        "                        # Add the statistical data for mpnn_complex\n",
        "                        for label in statistics_labels:\n",
        "                            mpnn_data.append(mpnn_complex_averages.get(label, None))\n",
        "                            for model in model_numbers:\n",
        "                                mpnn_data.append(mpnn_complex_statistics.get(model, {}).get(label, None))\n",
        "\n",
        "                        # Add the statistical data for binder\n",
        "                        for label in ['pLDDT', 'pTM', 'pAE', 'Binder_RMSD']:  # These are the labels for binder alone\n",
        "                            mpnn_data.append(binder_averages.get(label, None))\n",
        "                            for model in model_numbers:\n",
        "                                mpnn_data.append(binder_statistics.get(model, {}).get(label, None))\n",
        "\n",
        "                        # Add the remaining non-statistical data\n",
        "                        mpnn_data.extend([elapsed_mpnn_text, seq_notes, settings_file, filters_file, advanced_file])\n",
        "\n",
        "                        # insert data into csv\n",
        "                        insert_data(mpnn_csv, mpnn_data)\n",
        "\n",
        "                        # find best model number by pLDDT\n",
        "                        plddt_values = {i: mpnn_data[i] for i in range(11, 15) if mpnn_data[i] is not None}\n",
        "\n",
        "                        # Find the key with the highest value\n",
        "                        highest_plddt_key = int(max(plddt_values, key=plddt_values.get))\n",
        "\n",
        "                        # Output the number part of the key\n",
        "                        best_model_number = highest_plddt_key - 10\n",
        "                        best_model_pdb = os.path.join(design_paths[\"MPNN/Relaxed\"], f\"{mpnn_design_name}_model{best_model_number}.pdb\")\n",
        "\n",
        "                        # run design data against filter thresholds\n",
        "                        filter_conditions = check_filters(mpnn_data, design_labels, filters)\n",
        "                        if filter_conditions == True:\n",
        "                            print(mpnn_design_name+\" passed all filters\")\n",
        "                            accepted_mpnn += 1\n",
        "                            accepted_designs += 1\n",
        "\n",
        "                            # copy designs to accepted folder\n",
        "                            shutil.copy(best_model_pdb, design_paths[\"Accepted\"])\n",
        "\n",
        "                            # insert data into final csv\n",
        "                            final_data = [''] + mpnn_data\n",
        "                            insert_data(final_csv, final_data)\n",
        "\n",
        "                            # copy animation from accepted trajectory\n",
        "                            if advanced_settings[\"save_design_animations\"]:\n",
        "                                accepted_animation = os.path.join(design_paths[\"Accepted/Animation\"], f\"{design_name}.html\")\n",
        "                                if not os.path.exists(accepted_animation):\n",
        "                                    shutil.copy(os.path.join(design_paths[\"Trajectory/Animation\"], f\"{design_name}.html\"), accepted_animation)\n",
        "\n",
        "                            # copy plots of accepted trajectory\n",
        "                            plot_files = os.listdir(design_paths[\"Trajectory/Plots\"])\n",
        "                            plots_to_copy = [f for f in plot_files if f.startswith(design_name) and f.endswith('.png')]\n",
        "                            for accepted_plot in plots_to_copy:\n",
        "                                source_plot = os.path.join(design_paths[\"Trajectory/Plots\"], accepted_plot)\n",
        "                                target_plot = os.path.join(design_paths[\"Accepted/Plots\"], accepted_plot)\n",
        "                                if not os.path.exists(target_plot):\n",
        "                                    shutil.copy(source_plot, target_plot)\n",
        "\n",
        "                        else:\n",
        "                            print(f\"Unmet filter conditions for {mpnn_design_name}\")\n",
        "                            failure_df = pd.read_csv(failure_csv)\n",
        "                            special_prefixes = ('Average_', '1_', '2_', '3_', '4_', '5_')\n",
        "                            incremented_columns = set()\n",
        "\n",
        "                            for column in filter_conditions:\n",
        "                                base_column = column\n",
        "                                for prefix in special_prefixes:\n",
        "                                    if column.startswith(prefix):\n",
        "                                        base_column = column.split('_', 1)[1]\n",
        "\n",
        "                                if base_column not in incremented_columns:\n",
        "                                    failure_df[base_column] = failure_df[base_column] + 1\n",
        "                                    incremented_columns.add(base_column)\n",
        "\n",
        "                            failure_df.to_csv(failure_csv, index=False)\n",
        "                            shutil.copy(best_model_pdb, design_paths[\"Rejected\"])\n",
        "\n",
        "                        # increase MPNN design number\n",
        "                        mpnn_n += 1\n",
        "\n",
        "                        # if enough mpnn sequences of the same trajectory pass filters then stop\n",
        "                        if accepted_mpnn >= advanced_settings[\"max_mpnn_sequences\"]:\n",
        "                            break\n",
        "\n",
        "                    if accepted_mpnn >= 1:\n",
        "                        print(\"Found \"+str(accepted_mpnn)+\" MPNN designs passing filters\")\n",
        "                    else:\n",
        "                        print(\"No accepted MPNN designs found for this trajectory.\")\n",
        "\n",
        "                else:\n",
        "                    print('Duplicate MPNN designs sampled with different trajectory, skipping current trajectory optimisation')\n",
        "\n",
        "                # save space by removing unrelaxed design trajectory PDB\n",
        "                if advanced_settings[\"remove_unrelaxed_trajectory\"]:\n",
        "                    os.remove(trajectory_pdb)\n",
        "\n",
        "                # measure time it took to generate designs for one trajectory\n",
        "                design_time = time.time() - design_start_time\n",
        "                design_time_text = f\"{'%d hours, %d minutes, %d seconds' % (int(design_time // 3600), int((design_time % 3600) // 60), int(design_time % 60))}\"\n",
        "                print(\"Design and validation of trajectory \"+design_name+\" took: \"+design_time_text)\n",
        "\n",
        "            # analyse the rejection rate of trajectories to see if we need to readjust the design weights\n",
        "            if trajectory_n >= advanced_settings[\"start_monitoring\"] and advanced_settings[\"enable_rejection_check\"]:\n",
        "                acceptance = accepted_designs / trajectory_n\n",
        "                if not acceptance >= advanced_settings[\"acceptance_rate\"]:\n",
        "                    print(\"The ratio of successful designs is lower than defined acceptance rate! Consider changing your design settings!\")\n",
        "                    print(\"Script execution stopping...\")\n",
        "                    break\n",
        "\n",
        "        # increase trajectory number\n",
        "        trajectory_n += 1\n",
        "\n",
        "        # Colab-specific: update counters\n",
        "        num_sampled_trajectories = len(pd.read_csv(trajectory_csv))\n",
        "        num_accepted_designs = len(pd.read_csv(final_csv))\n",
        "        sampled_trajectories_label.value = f\"Sampled trajectories: {num_sampled_trajectories}\"\n",
        "        accepted_designs_label.value = f\"Accepted designs: {num_accepted_designs}\"\n",
        "\n",
        "### Script finished\n",
        "elapsed_time = time.time() - script_start_time\n",
        "elapsed_text = f\"{'%d hours, %d minutes, %d seconds' % (int(elapsed_time // 3600), int((elapsed_time % 3600) // 60), int(elapsed_time % 60))}\"\n",
        "print(\"Finished all designs. Script execution for \"+str(trajectory_n)+\" trajectories took: \"+elapsed_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36c6e1c7"
      },
      "source": [
        "### Fixing `NameError: name 'tau' is not defined` in `colabdesign_utils.py`\n",
        "\n",
        "The error occurs because `tau` is used within the `add_dual_ptme_softmax_loss` function in `/content/bindcraft/functions/colabdesign_utils.py` without being defined. However, `tau_init` is a valid parameter passed to this function. This cell will modify the file to use `tau_init` instead of `tau`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a6a4461"
      },
      "source": [
        "import fileinput\n",
        "import os\n",
        "\n",
        "file_path = '/content/bindcraft/functions/colabdesign_utils.py'\n",
        "\n",
        "# Read the content of the file\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Modify the specific line\n",
        "modified_lines = []\n",
        "for line in lines:\n",
        "    if 'self.opt[\"tau\"] = float(tau)' in line and not line.strip().startswith('#'):\n",
        "        modified_lines.append(line.replace('self.opt[\"tau\"] = float(tau)', 'self.opt[\"tau\"] = float(tau_init)'))\n",
        "    else:\n",
        "        modified_lines.append(line)\n",
        "\n",
        "# Write the modified content back to the file\n",
        "with open(file_path, 'w') as file:\n",
        "    file.writelines(modified_lines)\n",
        "\n",
        "print(f\"Successfully modified {file_path}. Please re-run the 'Run BindCraft!' cell.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auto-generated-id-1"
      },
      "source": [
        "import fileinput\n",
        "import os\n",
        "import re\n",
        "\n",
        "file_path_utils = '/content/colabdesign/shared/utils.py'\n",
        "\n",
        "# 1. Ensure 'import math' is at the top of the file\n",
        "with open(file_path_utils, 'r') as f:\n",
        "    utils_content = f.read()\n",
        "\n",
        "if \"import math\" not in utils_content:\n",
        "    utils_content = \"import math\\n\" + utils_content\n",
        "    with open(file_path_utils, 'w') as f:\n",
        "        f.write(utils_content)\n",
        "    print(f\"Added 'import math' to {file_path_utils}\")\n",
        "else:\n",
        "    print(f\"'import math' already exists in {file_path_utils}\")\n",
        "\n",
        "# 2. Modify the dict_to_str function to handle NaN values\n",
        "# This pattern looks for the specific structure of the 'if isinstance(v,float):' block\n",
        "old_pattern = re.compile(r\"\"\"(^\\s*)(if isinstance\\(v,float\\):) # group 1: indentation, group 2: the line\n",
        "(^\\s*)(      if int\\(v\\) == v:) # group 3: indentation, group 4: the line\n",
        "(^\\s*)(        print_str \\+= f\\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auto-generated-id-3"
      },
      "source": [
        "import fileinput\n",
        "import os\n",
        "import re\n",
        "\n",
        "file_path_utils = '/content/colabdesign/shared/utils.py'\n",
        "\n",
        "# Read the current content of the file\n",
        "with open(file_path_utils, 'r') as f:\n",
        "    utils_content = f.read()\n",
        "\n",
        "# Add 'import math' if it's not already there\n",
        "if \"import math\" not in utils_content:\n",
        "    utils_content = \"import math\\n\" + utils_content\n",
        "    print(f\"Added 'import math' to {file_path_utils}\")\n",
        "else:\n",
        "    print(f\"'import math' already exists in {file_path_utils}\")\n",
        "\n",
        "# Define the pattern to find and the replacement string\n",
        "# This pattern looks for the specific block where the ValueError occurs\n",
        "old_pattern = re.compile(r\"\"\"\n",
        "(^\\s*)(if isinstance\\(v,float\\):)\n",
        "(^\\s*)(      if int\\(v\\) == v:)\n",
        "(^\\s*)(        print_str \\+= f\\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdmYnBypaUHR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5qKDGGabhZo"
      },
      "outputs": [],
      "source": [
        "#@title Consolidate & Rank Designs\n",
        "#@markdown ---\n",
        "accepted_binders = [f for f in os.listdir(design_paths[\"Accepted\"]) if f.endswith('.pdb')]\n",
        "\n",
        "for f in os.listdir(design_paths[\"Accepted/Ranked\"]):\n",
        "    os.remove(os.path.join(design_paths[\"Accepted/Ranked\"], f))\n",
        "\n",
        "# load dataframe of designed binders\n",
        "design_df = pd.read_csv(mpnn_csv)\n",
        "# design_df = design_df.sort_values('Average_i_pTM', ascending=False)\n",
        "design_df = design_df.sort_values('Average_multi_ptme', ascending = True)\n",
        "\n",
        "# create final csv dataframe to copy matched rows, initialize with the column labels\n",
        "final_df = pd.DataFrame(columns=final_labels)\n",
        "\n",
        "# check the ranking of the designs and copy them with new ranked IDs to the folder\n",
        "rank = 1\n",
        "for _, row in design_df.iterrows():\n",
        "    for binder in accepted_binders:\n",
        "        target_settings[\"binder_name\"], model = binder.rsplit('_model', 1)\n",
        "        if target_settings[\"binder_name\"] == row['Design']:\n",
        "            # rank and copy into ranked folder\n",
        "            row_data = {'Rank': rank, **{label: row[label] for label in design_labels}}\n",
        "            final_df = pd.concat([final_df, pd.DataFrame([row_data])], ignore_index=True)\n",
        "            old_path = os.path.join(design_paths[\"Accepted\"], binder)\n",
        "            new_path = os.path.join(design_paths[\"Accepted/Ranked\"], f\"{rank}_{target_settings['binder_name']}_model{model.rsplit('.', 1)[0]}.pdb\")\n",
        "            shutil.copyfile(old_path, new_path)\n",
        "\n",
        "            rank += 1\n",
        "            break\n",
        "\n",
        "# save the final_df to final_csv\n",
        "final_df.to_csv(final_csv, index=False)\n",
        "\n",
        "print(\"Designs ranked and final_designs_stats.csv generated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ro9PQBE9zoIw"
      },
      "outputs": [],
      "source": [
        "#@title Top 20 Designs\n",
        "df = pd.read_csv(os.path.join(design_path, 'final_design_stats.csv'))\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qOtpEzCbzoX8"
      },
      "outputs": [],
      "source": [
        "#@title Top Design Display\n",
        "import py3Dmol\n",
        "import glob\n",
        "from IPython.display import HTML\n",
        "\n",
        "#### pymol top design\n",
        "top_design_dir = os.path.join(design_path, 'Accepted', 'Ranked')\n",
        "top_design_pdb = glob.glob(os.path.join(top_design_dir, '1_*.pdb'))[0]\n",
        "\n",
        "# Visualise in PyMOL\n",
        "view = py3Dmol.view()\n",
        "view.addModel(open(top_design_pdb, 'r').read(),'pdb')\n",
        "view.setBackgroundColor('white')\n",
        "view.setStyle({'chain':'A'}, {'cartoon': {'color':'#3c5b6f'}})\n",
        "view.setStyle({'chain':'B'}, {'cartoon': {'color':'#B76E79'}})\n",
        "view.zoomTo()\n",
        "view.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX0E849cdpTv"
      },
      "outputs": [],
      "source": [
        "#@title Display animation\n",
        "import glob\n",
        "from IPython.display import HTML\n",
        "\n",
        "#### pymol top design\n",
        "top_design_dir = os.path.join(design_path, 'Accepted', 'Ranked')\n",
        "top_design_pdb = glob.glob(os.path.join(top_design_dir, '1_*.pdb'))[0]\n",
        "\n",
        "top_design_name = os.path.basename(top_design_pdb).split('1_', 1)[1].split('_mpnn')[0]\n",
        "top_design_animation = os.path.join(design_path, 'Accepted', 'Animation', f\"{top_design_name}.html\")\n",
        "\n",
        "# Show animation\n",
        "HTML(top_design_animation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Quick Diagnostic Script for Zero multi_ptme Issue\n",
        "==================================================\n",
        "\n",
        "PASTE THIS CODE IMMEDIATELY AFTER af_model.prep_inputs() in your notebook\n",
        "(around line 38-48 in colabdesign_utils.py, in binder_hallucination function)\n",
        "\"\"\"\n",
        "\n",
        "# ==================== DIAGNOSTIC CODE START ====================\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🔍 DUAL pTME DIAGNOSTIC REPORT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Check if inputs are prepared\n",
        "print(\"\\n1️⃣ Checking af_model._inputs...\")\n",
        "if hasattr(af_model, '_inputs'):\n",
        "    inputs = af_model._inputs\n",
        "    print(f\"   ✅ _inputs exists with {len(inputs)} keys\")\n",
        "    print(f\"   Keys: {list(inputs.keys())}\")\n",
        "else:\n",
        "    print(f\"   ❌ _inputs NOT FOUND - this is a problem!\")\n",
        "    inputs = None\n",
        "\n",
        "# 2. Check for chain_index\n",
        "print(\"\\n2️⃣ Checking chain_index...\")\n",
        "if inputs and 'chain_index' in inputs:\n",
        "    ci = np.array(inputs['chain_index'])\n",
        "    unique, counts = np.unique(ci, return_counts=True)\n",
        "    print(f\"   ✅ chain_index FOUND!\")\n",
        "    print(f\"   Unique chain IDs: {unique}\")\n",
        "    print(f\"   Residue counts per chain: {counts}\")\n",
        "    print(f\"   Total residues: {len(ci)}\")\n",
        "    print(f\"   First 10 values: {ci[:10]}\")\n",
        "    print(f\"   Last 10 values: {ci[-10:]}\")\n",
        "\n",
        "    # Diagnose the structure\n",
        "    print(\"\\n   📊 Structure Analysis:\")\n",
        "    if len(unique) == 2:\n",
        "        print(f\"   ⚠️  Found 2 chains (expected 3 for dual-target)\")\n",
        "        print(f\"      → Chain 0: {counts[0]} residues (probably both targets merged)\")\n",
        "        print(f\"      → Chain 1: {counts[1]} residues (probably binder)\")\n",
        "    elif len(unique) == 3:\n",
        "        print(f\"   ✅ Found 3 chains (good!)\")\n",
        "        print(f\"      → Chain 0: {counts[0]} residues\")\n",
        "        print(f\"      → Chain 1: {counts[1]} residues\")\n",
        "        print(f\"      → Chain 2: {counts[2]} residues\")\n",
        "    else:\n",
        "        print(f\"   ⚠️  Found {len(unique)} chains (unexpected)\")\n",
        "else:\n",
        "    print(f\"   ❌ chain_index NOT FOUND\")\n",
        "    if inputs:\n",
        "        print(f\"   Available input keys: {list(inputs.keys())}\")\n",
        "        # Try alternative keys\n",
        "        for alt_key in ['asym_id', 'asym_index', 'entity_id']:\n",
        "            if alt_key in inputs:\n",
        "                print(f\"   ℹ️  Found alternative: '{alt_key}'\")\n",
        "                alt_ci = np.array(inputs[alt_key])\n",
        "                unique_alt = np.unique(alt_ci)\n",
        "                print(f\"      Unique values: {unique_alt}\")\n",
        "\n",
        "# 3. Check target and binder lengths\n",
        "print(\"\\n3️⃣ Checking model structure...\")\n",
        "if hasattr(af_model, '_target_len'):\n",
        "    print(f\"   ✅ _target_len: {af_model._target_len}\")\n",
        "else:\n",
        "    print(f\"   ❌ _target_len NOT SET\")\n",
        "\n",
        "if hasattr(af_model, '_binder_len'):\n",
        "    print(f\"   ✅ _binder_len: {af_model._binder_len}\")\n",
        "else:\n",
        "    print(f\"   ❌ _binder_len NOT SET\")\n",
        "\n",
        "# 4. Check your dual pTME configuration\n",
        "print(\"\\n4️⃣ Checking your dual pTME configuration...\")\n",
        "print(f\"   Config chains_A: {advanced_settings.get('dual_ptme_chains_A', 'NOT SET')}\")\n",
        "print(f\"   Config chains_B: {advanced_settings.get('dual_ptme_chains_B', 'NOT SET')}\")\n",
        "print(f\"   Config binder_chain_id: {advanced_settings.get('dual_ptme_binder_chain_id', 'NOT SET')}\")\n",
        "\n",
        "# 5. Analyze compatibility\n",
        "print(\"\\n5️⃣ Configuration vs Reality Check...\")\n",
        "if inputs and 'chain_index' in inputs:\n",
        "    ci = np.array(inputs['chain_index'])\n",
        "    unique_chains = np.unique(ci)\n",
        "\n",
        "    config_A = advanced_settings.get('dual_ptme_chains_A', [0])\n",
        "    config_B = advanced_settings.get('dual_ptme_chains_B', [1])\n",
        "    config_binder = advanced_settings.get('dual_ptme_binder_chain_id', 2)\n",
        "\n",
        "    print(f\"   Expected chains in config: {set(config_A + config_B + [config_binder])}\")\n",
        "    print(f\"   Actual chains in structure: {set(unique_chains)}\")\n",
        "\n",
        "    missing = set(config_A + config_B + [config_binder]) - set(unique_chains)\n",
        "    if missing:\n",
        "        print(f\"   ❌ MISMATCH! Missing chains: {missing}\")\n",
        "        print(f\"   This is why multi_ptme is zero!\")\n",
        "    else:\n",
        "        print(f\"   ✅ All configured chains exist\")\n",
        "\n",
        "# 6. Proposed solution\n",
        "print(\"\\n6️⃣ RECOMMENDED SOLUTION:\")\n",
        "if inputs and 'chain_index' in inputs:\n",
        "    ci = np.array(inputs['chain_index'])\n",
        "    unique, counts = np.unique(ci, return_counts=True)\n",
        "\n",
        "    if len(unique) == 2:\n",
        "        print(\"   Your structure has 2 chains (targets merged + binder)\")\n",
        "        print(\"   Solution A: Use residue-based dual pTME\")\n",
        "        print(\"   -------------------------------------------\")\n",
        "        print(\"   # In your colabdesign_utils.py, replace add_dual_ptme_softmax_loss call with:\")\n",
        "        print(\"   add_dual_ptme_residue_based(\")\n",
        "        print(\"       af_model,\")\n",
        "        if hasattr(af_model, '_target_len'):\n",
        "            target_len = af_model._target_len\n",
        "            # Assuming your dual_target.pdb: Chain A=192, Chain B=143\n",
        "            # This is from your notebook output\n",
        "            print(f\"       target_A_start=0,\")\n",
        "            print(f\"       target_A_end=191,      # Adjust based on your actual PDB\")\n",
        "            print(f\"       target_B_start=192,\")\n",
        "            print(f\"       target_B_end={target_len-1},\")\n",
        "        else:\n",
        "            print(f\"       target_A_start=0,\")\n",
        "            print(f\"       target_A_end=???,      # YOUR CHAIN A LENGTH - 1\")\n",
        "            print(f\"       target_B_start=???,    # CHAIN A LENGTH\")\n",
        "            print(f\"       target_B_end=???,      # TOTAL TARGET LENGTH - 1\")\n",
        "        print(\"       weight=0.05,\")\n",
        "        print(\"       tau=0.2\")\n",
        "        print(\"   )\")\n",
        "        print()\n",
        "        print(\"   OR\")\n",
        "        print()\n",
        "        print(\"   Solution B: Update JSON config to match reality\")\n",
        "        print(\"   -----------------------------------------------\")\n",
        "        print(\"   {\")\n",
        "        print(f'       \"dual_ptme_chains_A\": [0],    // Target (merged)')\n",
        "        print(f'       \"dual_ptme_chains_B\": [0],    // Same chain!')\n",
        "        print(f'       \"dual_ptme_binder_chain_id\": {unique[-1]},  // Binder')\n",
        "        print(\"       // Then modify loss function to use residue ranges\")\n",
        "        print(\"   }\")\n",
        "\n",
        "    elif len(unique) == 3:\n",
        "        print(\"   Your structure has 3 chains - config should work!\")\n",
        "        print(\"   Current config:\")\n",
        "        print(f\"   {{\")\n",
        "        print(f'       \"dual_ptme_chains_A\": {config_A},')\n",
        "        print(f'       \"dual_ptme_chains_B\": {config_B},')\n",
        "        print(f'       \"dual_ptme_binder_chain_id\": {config_binder}')\n",
        "        print(f\"   }}\")\n",
        "        print()\n",
        "        print(\"   Suggested config based on structure:\")\n",
        "        print(f\"   {{\")\n",
        "        print(f'       \"dual_ptme_chains_A\": [{unique[0]}],')\n",
        "        print(f'       \"dual_ptme_chains_B\": [{unique[1]}],')\n",
        "        print(f'       \"dual_ptme_binder_chain_id\": {unique[2]}')\n",
        "        print(f\"   }}\")\n",
        "else:\n",
        "    print(\"   ❌ Cannot determine solution - chain_index not available\")\n",
        "    print(\"   You may need to check:\")\n",
        "    print(\"   1. Is use_multimer_design: true in your config?\")\n",
        "    print(\"   2. Is prep_inputs being called correctly?\")\n",
        "    print(\"   3. Try running af_model._prep_features() manually\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🔍 END OF DIAGNOSTIC REPORT\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# ==================== DIAGNOSTIC CODE END ====================\n",
        "\n",
        "\"\"\"\n",
        "WHAT TO DO NEXT:\n",
        "================\n",
        "\n",
        "1. Copy the diagnostic code above\n",
        "2. Paste it in your notebook AFTER line 38 (after af_model.prep_inputs)\n",
        "3. Run ONE iteration of your design\n",
        "4. Read the diagnostic output carefully\n",
        "5. Apply the recommended solution\n",
        "\n",
        "The diagnostic will tell you EXACTLY what's wrong and how to fix it!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Lf-Mytz5dsZr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ff7d39e211514b43bc64bf39a422c3a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b87c8a60a89479085437fa7414cfc57",
              "IPY_MODEL_a9b21b0c30d644669ebf9fc1bf4fbc53"
            ],
            "layout": "IPY_MODEL_bd842dacd993410f936fbdfc549b2db8"
          }
        },
        "2b87c8a60a89479085437fa7414cfc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd6626b22753425dba9a91ced5ae562f",
            "placeholder": "​",
            "style": "IPY_MODEL_db2ec6b24eb44a53b1a8bb36c3a11f5a",
            "value": "<h3 style='color: #1f77b4;'>Sampled Trajectories: <span style='color: #1f77b4;'>2</span></h3>"
          }
        },
        "a9b21b0c30d644669ebf9fc1bf4fbc53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_916c0b73c7b64624a982b7da54df1381",
            "placeholder": "​",
            "style": "IPY_MODEL_455fd818088a4085a4d03d8807005172",
            "value": "<h3 style='color: #2ca02c;'>Accepted Designs: <span style='color: #2ca02c;'>0</span></h3>"
          }
        },
        "bd842dacd993410f936fbdfc549b2db8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd6626b22753425dba9a91ced5ae562f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db2ec6b24eb44a53b1a8bb36c3a11f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "916c0b73c7b64624a982b7da54df1381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "455fd818088a4085a4d03d8807005172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}